{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import configparser\n",
    "from itertools import chain\n",
    "from subprocess import Popen, PIPE\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "# import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from utils_data import remap_labels\n",
    "from utils_tensor import sort_variables_lengths, sort_variables_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object for input (words) and output (labels) sequence \n",
    "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called Sequence which has word → index (word2index) and index → word (index2word) dictionaries, as well as a count of each word (word2count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequence:\n",
    "    def __init__(self, name, is_output):\n",
    "        self.name = name\n",
    "        self.is_output = is_output\n",
    "        if is_output:            \n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.words = []\n",
    "            self.n_words = 0\n",
    "\n",
    "        else:\n",
    "            self.word2idx = {'<UNK>':1}\n",
    "            self.idx2word = {1:'<UNK>'}\n",
    "            self.words = ['<UNK>']\n",
    "            self.n_words = 1\n",
    "            \n",
    "            # creating alphabet dict for character embedding\n",
    "            self.char2idx = {}\n",
    "            self.idx2char = {}\n",
    "            self.chars = []\n",
    "            self.n_chars = 0\n",
    "            self.max_word_len = 0\n",
    "            self.max_word = ''\n",
    "        \n",
    "        self.word_count = {}\n",
    "        self.unk_list = []\n",
    "        self.char_count = {}\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "                self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.words:\n",
    "            self.words.append(word)\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "\n",
    "        else:\n",
    "            self.word_count[word] += 1\n",
    "        \n",
    "        if not self.is_output:\n",
    "            word_len = 0\n",
    "            for char in word:\n",
    "                self.add_char(char)\n",
    "                word_len+=1\n",
    "                if word_len > self.max_word_len:\n",
    "                    self.max_word_len = word_len\n",
    "                    self.max_word = word\n",
    "                    \n",
    "    def add_char(self, char):\n",
    "        if char not in self.chars:\n",
    "            self.chars.append(char)\n",
    "            self.char_count[char] = 1\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char_count[char] += 1\n",
    "            \n",
    "    def make_char_dict(self):\n",
    "        for idx, char in enumerate(self.chars):\n",
    "            self.char2idx[char] = idx + 1\n",
    "            self.idx2char[idx+1] = char\n",
    "            \n",
    "    def make_word_dict(self, is_output = False, shuffle = True):\n",
    "        \n",
    "        if shuffle:\n",
    "            if is_output:\n",
    "                preserved_tags = self.words[:1]\n",
    "                actual_words = self.words[1:]\n",
    "                random.shuffle(actual_words)\n",
    "                self.words = preserved_tags + actual_words\n",
    "\n",
    "            else:\n",
    "                preserved_tags = self.words[:3]\n",
    "                actual_words = self.words[3:]\n",
    "                random.shuffle(actual_words)\n",
    "                self.words = preserved_tags + actual_words\n",
    "                \n",
    "        for idx, word in enumerate(self.words):\n",
    "            self.word2idx[word] = idx + 1\n",
    "            self.idx2word[idx+1] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn Unicode characters to ASCII\n",
    "def unicode2ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode2ascii(s.strip())\n",
    "    # substituting digits with 0, but keep the format\n",
    "    s = re.sub(r\"\\d\", r\"0\", s)\n",
    "    # removing non-letter chars:\n",
    "    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(dataset_filepath, data_obj_file, resume_training = False):\n",
    "    \"\"\"\n",
    "    reading data (train, valid and test) file, spliting to words and labels pairs\n",
    "    return:\n",
    "    all_pairs: dictionary ['train']['valid']['test'] containing lists of sentence and label sequence pairs.\n",
    "               e.g.: [u'SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .',\n",
    "                      u'O O O O O O O B-PER O O O O']        \n",
    "    \"\"\"\n",
    "    # if train from scratch, read sequences from dataset files\n",
    "    if resume_training == False:\n",
    "        print('Reading files...\\nInitiating Sequence objects...')\n",
    "        input_seq = Sequence('sentences', is_output=False)\n",
    "        output_seq = Sequence('labels', is_output=True)\n",
    "\n",
    "    #resume training from a checkpoint, read sequences from saved object\n",
    "    else:\n",
    "        print('Resume training...\\nLoading Sequence objects... ')\n",
    "        (input_seq, output_seq, _) = load_data_from_file(data_obj_file)\n",
    "\n",
    "    all_pairs = {}\n",
    "    for dataset_type in ['train', 'valid', 'test']:\n",
    "        data_filepath = os.path.join(dataset_filepath, dataset_type + '.txt')\n",
    "        print('reading lines in {:s} file (path: {:s})'.format(dataset_type, data_filepath))\n",
    "        pairs = []\n",
    "        for line in open(data_filepath):\n",
    "            pairs.append([normalize_string(s) for s in line.strip().split('\\t')])\n",
    "        all_pairs[dataset_type] = pairs\n",
    "        print('Read {:d} pairs in {:s} set'.format(len(all_pairs[dataset_type]), dataset_type))\n",
    "\n",
    "    return input_seq, output_seq, all_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "- Read text file and split into lines  \n",
    "- Split lines into pairs and normalize  \n",
    "- Make word (or label, char) lists from sentences in pairs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare Data\n",
    "def prepare_data(dataset_filepath, data_obj_file, resume_training = False):\n",
    "    '''\n",
    "    Prepare data for train, valid and test\n",
    "    :param dataset_filepath: str \n",
    "    :param dataset_type: str, train/valid/test set\n",
    "    :param data_obj_file: str, save path for the prepared data object\n",
    "    :param resume_training: bool, if resume training from a check point\n",
    "    :return:\n",
    "    '''\n",
    "    senteces, labels, all_pairs = read_files(dataset_filepath, data_obj_file, resume_training)\n",
    "    # train_pairs, val_pair, test_pairs = pairs\n",
    "    all_lengths = {}\n",
    "    all_max_len = {} # for the rnn.pack_padded_sequence\n",
    "    \n",
    "    for dataset_type, pairs in all_pairs.iteritems():\n",
    "        all_lengths[dataset_type] = [len(pairs[x][0].strip().split(' ')) for x in range(len(pairs))]\n",
    "        all_max_len[dataset_type] = max(all_lengths[dataset_type])\n",
    "        print('Counting sentence length in {:s} set...'.format(dataset_type))\n",
    "        print(\"For {:s} set, maximum length of sentence sentence : {:d}\".format(dataset_type, all_max_len[dataset_type]))\n",
    "\n",
    "    for dataset_type in all_pairs.keys():\n",
    "        if not resume_training:\n",
    "            print('Processing {:s} set, adding words to the dictionary...'.format(dataset_type))\n",
    "            for pair in all_pairs[dataset_type]:\n",
    "                senteces.add_sentence(pair[0])\n",
    "                labels.add_sentence(pair[1])\n",
    "            # break here to see the senteces.chars char2idx idx2char\n",
    "            print('shuffle word dictionary...')\n",
    "            senteces.make_word_dict(is_output=False)\n",
    "            senteces.make_char_dict()\n",
    "            labels.make_word_dict(is_output=True)\n",
    "            print('shuffle char dictionary...')\n",
    "            senteces.make_char_dict()\n",
    "            print('Done!')\n",
    "            print('Saving Sequence objects to file {:s}'.format(data_obj_file))\n",
    "            save_data_to_file(senteces, labels, all_pairs, data_obj_file)\n",
    "        else:\n",
    "            print('Resume training, not adding any words to Lang objects.')\n",
    "            \n",
    "    print(\"Counted vocab size:\")\n",
    "    print(senteces.name, senteces.n_words)\n",
    "    print(labels.name, labels.n_words)\n",
    "    print(labels.idx2word)\n",
    "    print('idx2word for senteces:')\n",
    "    print([senteces.idx2word[i] for i in range(1, 31)])\n",
    "    print('idx2char for senteces:')\n",
    "    print([senteces.idx2char[i] for i in range(1,31)])\n",
    "    return senteces, labels, all_pairs, all_max_len, all_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data_to_file(input_seq, output_seq, pairs, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump((input_seq,output_seq, pairs), f)\n",
    "    print('saved Sequence object to {:s}'.format(fname))\n",
    "\n",
    "def load_data_from_file(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data= pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning training data into Tensors\n",
    "To train we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a LongTensor which represents the index (from the Sequence indexes made earlier) of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(input_seq, output_seq, pairs, batch_size, max_len, lengths):\n",
    "    data_size = len(pairs)\n",
    "    num_batches = int((data_size - 1)/batch_size) + 1\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        yield variables_from_pairs(input_seq, output_seq, pairs[start_index:end_index], max_len), lengths[start_index:end_index]\n",
    "\n",
    "def variables_from_pairs(input_seq, output_seq, pairs, max_len):\n",
    "    x_variables = []\n",
    "    y_variables = []\n",
    "    x_variables_char = []\n",
    "    for pair in pairs:\n",
    "        # get varaible representing sentence sequence, label sequence and character sequence\n",
    "        # from an input/output pair\n",
    "        x_variable, y_variable, x_variable_char = variables_from_pair(input_seq, output_seq,\n",
    "                                                                      pair, max_len)\n",
    "        x_variables.append(x_variable)\n",
    "        y_variables.append(y_variable)\n",
    "        x_variables_char.append(x_variable_char)\n",
    "    return x_variables, y_variables , x_variables_char\n",
    "\n",
    "def variables_from_pair(input_seq, output_seq, pair, max_len):\n",
    "    '''\n",
    "    Given an input/output pair, turn them into pytorch Variable\n",
    "    :param input_seq: Sequence object\n",
    "    :param output_seq: Sequence object\n",
    "    :param pair: list of str\n",
    "    :param max_len: int\n",
    "    :return: varaible representing sentence sequence, label sequence and character sequence\n",
    "    '''    \n",
    "    input_variable = variable_from_sentence(input_seq, pair[0], max_len)\n",
    "    target_variable = variable_from_sentence(output_seq, pair[1], max_len)\n",
    "    input_variable_char = []\n",
    "    cnt = 0\n",
    "    #getting char variable:\n",
    "    for word in pair[0].split(' '):\n",
    "        input_variable_char.append(variable_from_word(input_seq, word, input_seq.max_word_len))\n",
    "        cnt += 1\n",
    "    new_var = [0] * input_seq.max_word_len\n",
    "    for i in range(max_len - cnt): #make input_variable_char the length of max_len\n",
    "        input_variable_char.append(new_var)\n",
    "        # input_variable_char should be list of list\n",
    "    return (input_variable, target_variable, input_variable_char)\n",
    "    \n",
    "def variable_from_sentence(sequence, sentence, max_len):\n",
    "    '''\n",
    "    :param sequence: Sequence object\n",
    "    :param sentence: str, a sentence, each word seperated by ' '\n",
    "    :param max_len: int, max len of the sentence (i.e. max len of the returned list),\n",
    "                    sentences which has less words than max len are padded with 0 in the end.\n",
    "    :return result: list of int, length: max_len\n",
    "    '''\n",
    "    indexes = indexes_from_sentence(sequence, sentence) # a sentence as indices.\n",
    "    if len(indexes) < max_len:\n",
    "        indexes = indexes + [0] * (max_len - len(indexes))\n",
    "    return indexes\n",
    "\n",
    "def indexes_from_sentence(sequence, sentence):\n",
    "    '''\n",
    "    :param sequence: Sequence object\n",
    "    :param sentence: str, a sentence, each word seperated by ' '\n",
    "    :return idxes: int list, words in sentence mapped to indexes\n",
    "    '''\n",
    "    idxes = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            idx = sequence.word2idx[word]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                idx = sequence.word2idx['<UNK>']\n",
    "                sequence.unk_list.append(word)\n",
    "            except KeyError:\n",
    "                print('keyerror: {:s}'.format(word))\n",
    "        idxes.append(idx)\n",
    "    return idxes\n",
    "\n",
    "def variable_from_word(sequence, word, max_word_len):\n",
    "    '''\n",
    "    :param sequence: Sequence object\n",
    "    :param word: str, a word\n",
    "    :param max_word_len: int, max len of a word\n",
    "    :return indexes: list of int, length: max_len\n",
    "    '''\n",
    "    indexes = indexes_from_word(sequence, word)\n",
    "    if len(indexes) < max_word_len:\n",
    "        indexes = indexes + [0] * (max_word_len - len(indexes))\n",
    "    return indexes\n",
    "\n",
    "def indexes_from_word(sequence, word):\n",
    "    '''\n",
    "    :param sequence: Sequence object\n",
    "    :param word: str, a word\n",
    "    :return idxes: list of int, each char in the word mapped to indexes\n",
    "    '''\n",
    "    idxes = []\n",
    "    for char in word:\n",
    "        idx = sequence.char2idx[char]\n",
    "        idxes.append(idx)\n",
    "    return idxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pretrained_word_embeddings(parameters, input_seq):\n",
    "    emb_file = parameters['embedding_filepath']\n",
    "    f = open(emb_file, 'r')\n",
    "    line = f.readline()\n",
    "    emb_size = len(line.strip().split(' ')) - 1\n",
    "    f.close()\n",
    "    print('loading word embeddings from file {:s}...\\n Embedding size: {:d}'.format(\n",
    "        emb_file, emb_size\n",
    "    ))\n",
    "    \n",
    "    embbedding_weights = np.zeros((input_seq.n_words + 1, emb_size))\n",
    "    pretrained_embeddings = {}\n",
    "    with open(emb_file, 'r') as f:\n",
    "        for line in f:\n",
    "            splited = line.strip().split(' ')\n",
    "            if len(splited) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                pretrained_embeddings[splited[0]] = splited[1:]\n",
    "    direct_map = 0\n",
    "    lowercase_map = 0\n",
    "    random_init = 0\n",
    "    map_to_unk = 0\n",
    "    low_frequency_word = []\n",
    "    others = []\n",
    "    words_without_pretrained_embeddings = []\n",
    "    for word in input_seq.words:\n",
    "        if word in ['<SOS>', '<EOS>', '<UNK>']:\n",
    "            continue\n",
    "        elif word in pretrained_embeddings:\n",
    "            vector = np.array(pretrained_embeddings[word], dtype=float)\n",
    "            embbedding_weights[input_seq.word2idx[word]] = vector\n",
    "            direct_map += 1\n",
    "        elif word.lower() in pretrained_embeddings:\n",
    "            vector = np.array(pretrained_embeddings[word.lower()], dtype=float)\n",
    "            embbedding_weights[input_seq.word2idx[word]] = vector\n",
    "            lowercase_map += 1\n",
    "        elif input_seq.word_count[word] > 1:\n",
    "            # not low frequency word, but in\n",
    "            #random init\n",
    "            vector = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "            embbedding_weights[input_seq.word2idx[word]] = vector\n",
    "            random_init += 1\n",
    "        elif input_seq.word_count[word] <= 1:\n",
    "            low_frequency_word.append(word)\n",
    "        else:\n",
    "            others.append(word)\n",
    "    \n",
    "    print('Map {:d} tokens with pretrained embeddings.'.format(direct_map+lowercase_map))\n",
    "    print('direct map: {:d}\\nlower-case map: {:d}\\n'.format(direct_map, lowercase_map))\n",
    "    print('Randomly initialized {:d} token embeddings.'.format(random_init))\n",
    "    print('{:d} low_frequency_word: '.format(len(low_frequency_word)))\n",
    "    \n",
    "    return embbedding_weights, emb_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some related parameters / hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_epochs': 200,\n",
    "    'patience': 20,\n",
    "    'save_best_epoch': True,\n",
    "    'resume': False,\n",
    "    'batch_size': 128,\n",
    "    'clip': 5,\n",
    "    'use_pretrained_word_embedding': True,\n",
    "    'use_char_embedding': True,\n",
    "    'char_emb_size': 25,\n",
    "    'hidden_size': 128,\n",
    "    'n_layers': 1,\n",
    "    'dropout_p': 0.5,\n",
    "    'learning_rate': 0.05,\n",
    "    'lang_in': 'sent',\n",
    "    'lang_out': 'ner',\n",
    "    'output_model_dir': '/home/liah/ner/seq2seq_for_ner/src/model/tutorial', \n",
    "    'pred_output_dir': '/home/liah/ner/seq2seq_for_ner/src/result/tutorial',\n",
    "    'dataset_filepath' : '/home/liah/ner/seq2seq_for_ner/src/data/conll03-ner-org-bioes-5k/train1',\n",
    "    'baseline_folder': 'none',\n",
    "    'data_obj_file_name': 'obj_sent_ner_step1',\n",
    "    'embedding_filepath': '/home/liah/word_vectors/eng/glove.6B.100d.txt',\n",
    "    'model_path': ''\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Initiating Sequence objects...\n",
      "reading lines in train file (path: /home/liah/ner/seq2seq_for_ner/src/data/conll03-ner-org-bioes-5k/train1/train.txt)\n",
      "Read 5000 pairs in train set\n",
      "reading lines in valid file (path: /home/liah/ner/seq2seq_for_ner/src/data/conll03-ner-org-bioes-5k/train1/valid.txt)\n",
      "Read 3250 pairs in valid set\n",
      "reading lines in test file (path: /home/liah/ner/seq2seq_for_ner/src/data/conll03-ner-org-bioes-5k/train1/test.txt)\n",
      "Read 3453 pairs in test set\n",
      "Counting sentence length in test set...\n",
      "For test set, maximum length of sentence sentence : 124\n",
      "Counting sentence length in train set...\n",
      "For train set, maximum length of sentence sentence : 78\n",
      "Counting sentence length in valid set...\n",
      "For valid set, maximum length of sentence sentence : 109\n",
      "Processing test set, adding words to the dictionary...\n",
      "shuffle word dictionary...\n",
      "shuffle char dictionary...\n",
      "Done!\n",
      "Saving Sequence objects to file /home/liah/ner/seq2seq_for_ner/src/model/tutorial/obj_sent_ner_step1\n",
      "saved Sequence object to /home/liah/ner/seq2seq_for_ner/src/model/tutorial/obj_sent_ner_step1\n",
      "Processing train set, adding words to the dictionary...\n",
      "shuffle word dictionary...\n",
      "shuffle char dictionary...\n",
      "Done!\n",
      "Saving Sequence objects to file /home/liah/ner/seq2seq_for_ner/src/model/tutorial/obj_sent_ner_step1\n",
      "saved Sequence object to /home/liah/ner/seq2seq_for_ner/src/model/tutorial/obj_sent_ner_step1\n",
      "Processing valid set, adding words to the dictionary...\n",
      "shuffle word dictionary...\n",
      "shuffle char dictionary...\n",
      "Done!\n",
      "Saving Sequence objects to file /home/liah/ner/seq2seq_for_ner/src/model/tutorial/obj_sent_ner_step1\n",
      "saved Sequence object to /home/liah/ner/seq2seq_for_ner/src/model/tutorial/obj_sent_ner_step1\n",
      "Counted vocab size:\n",
      "sentences 19091\n",
      "labels 13\n",
      "{1: u'O', 2: u'S-LOC', 3: u'E-LOC', 4: u'B-LOC', 5: u'S-PER', 6: u'S-MISC', 7: u'B-PER', 8: u'B-MISC', 9: u'I-MISC', 10: u'E-MISC', 11: u'I-LOC', 12: u'E-PER', 13: u'I-PER'}\n",
      "idx2word for senteces:\n",
      "[u'<UNK>', u'SOCCER', u'-', u'Freiburg', u'Internacional', u'succeeded', u'lbs', u'Galeforce', u'Castellani', u'Dohuk', u'total', u'offsetting', u'headbutting', u'crimes', u'FEATURE', u'deputies', u'import', u'brand', u'mobile', u'deft', u'Disappearance', u'Sang', u'Suwon', u'Cork', u'Market-making', u'RIF', u'following', u'IVAC', u'shoulder', u'McGilley']\n",
      "idx2char for senteces:\n",
      "[u'S', u'O', u'C', u'E', u'R', u'-', u'J', u'A', u'P', u'N', u'G', u'T', u'L', u'U', u'K', u'Y', u'W', u'I', u',', u'H', u'D', u'F', u'.', u'a', u'd', u'i', u'm', u'k', u'n', u't']\n",
      "Example pairs:\n",
      "\tExample test sentence sequence:  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\n",
      "\tExample test label sequence:  O O S-LOC O O O O S-PER O O O O\n",
      "\tExample train sentence sequence:  Tirana 's national library has three copies of the \" Book of Mass ' . \"\n",
      "\tExample train label sequence:  S-LOC O O O O O O O O O B-MISC I-MISC E-MISC O O O\n",
      "\tExample valid sentence sequence:  CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "\tExample valid label sequence:  O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data_obj_file = os.path.join(parameters['output_model_dir'], parameters['data_obj_file_name'])\n",
    "\n",
    "input_seq, output_seq, \\\n",
    "all_pairs, all_max_len, all_lengths = prepare_data(dataset_filepath =parameters['dataset_filepath'],\n",
    "                                               data_obj_file=data_obj_file)\n",
    "print('Example pairs:')\n",
    "for data_type in all_max_len.keys():\n",
    "    print('\\tExample {:s} sentence sequence: '.format(data_type), all_pairs[data_type][0][0])\n",
    "    print('\\tExample {:s} label sequence: '.format(data_type), all_pairs[data_type][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectioanl LSTM model for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size,\n",
    "                 n_layers=1, dropout=0.1, pretrained_weights=None, output_size = 10,\n",
    "                 use_char_embedding = False, char_alphabet_size = 0, char_emb_size = 25):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout\n",
    "        self.use_char_embedding = use_char_embedding\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, emb_size)\n",
    "        if pretrained_weights is not None:\n",
    "            weights = torch.from_numpy(pretrained_weights).type(torch.FloatTensor)\n",
    "            assert weights.size(0) == input_size and weights.size(1) == emb_size\n",
    "            weights = weights.cuda() if use_cuda else weights\n",
    "            self.embedding.weight = nn.Parameter(weights)\n",
    "        \n",
    "        # use character embedding\n",
    "        if self.use_char_embedding and char_alphabet_size != 0:\n",
    "            self.char_alphabet_size = char_alphabet_size\n",
    "            self.char_emb_size = char_emb_size\n",
    "            self.char_embedding = nn.Embedding(self.char_alphabet_size, self.char_emb_size)\n",
    "            \n",
    "            self.char_lstm = nn.LSTM(char_emb_size, char_emb_size,\n",
    "                                     n_layers, dropout = 0.1,\n",
    "                                     bidirectional=True)\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        if not self.use_char_embedding:\n",
    "            self.lstm = nn.LSTM(emb_size, hidden_size,\n",
    "                            n_layers, dropout=0.1,\n",
    "                            bidirectional=True)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(emb_size + char_emb_size * 2, hidden_size,\n",
    "                                n_layers, dropout=0.1,\n",
    "                                bidirectional=True)\n",
    "\n",
    "        self.output_size = output_size\n",
    "        # self.fc = nn.Linear(2 * self.hidden_size, self.hidden_size)\n",
    "        self.out1 = nn.Linear(2 * self.hidden_size, self.hidden_size)\n",
    "        self.out2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "        # self.hidden = self.init_hidden(100)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None, input_seqs_char = None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        if self.use_char_embedding and input_seqs_char is not None:\n",
    "            char_batch_size = input_seqs_char.size(1)\n",
    "            char_seq_len = input_seqs_char.size(0)\n",
    "            input_seqs_char = input_seqs_char.view(char_seq_len * char_batch_size, -1)\n",
    "            char_embedded = self.char_embedding(input_seqs_char).transpose(1, 0)\n",
    "            char_lstm_output, (char_lstm_hidden, char_lstm_cell) = self.char_lstm(char_embedded)\n",
    "            char_lstm_output_sum = torch.cat((char_lstm_hidden[0], char_lstm_hidden[1]), -1)\n",
    "            char_embedded_seq = char_lstm_output_sum.view(char_seq_len, char_batch_size, -1)\n",
    "            embedded = torch.cat((embedded, char_embedded_seq), -1)\n",
    "                \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n",
    "\n",
    "        seq_len = outputs.size(0)\n",
    "        batch_size = outputs.size(1)\n",
    "        raw_outputs1 = self.out1(outputs.view(seq_len * batch_size, -1))\n",
    "        raw_outputs2 = self.out2(raw_outputs1).view(seq_len, batch_size, -1)\n",
    "        scores = self.softmax(raw_outputs2.transpose(2, 0))\n",
    "        scores = scores.transpose(2, 0)\n",
    "\n",
    "        return scores, hidden, output_lengths\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        c_state = Variable(torch.randn(self.n_layers * 2, batch_size, self.hidden_size))\n",
    "        h_state = Variable(torch.randn(self.n_layers * 2, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            c_state = c_state.cuda()\n",
    "            h_state = h_state.cuda()\n",
    "        return (c_state, h_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A training step (for a batch of examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(parameters, input_seq, input_variable, target_variable, lengths_sorted,\n",
    "               model, optimizer, criterion, eval_during_train =True,\n",
    "               input_variable_char=None):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    input_batch_size, input_seq_len = input_variable.size()[1], input_variable.size()[0]\n",
    "    target_batch_size, target_seq_len = target_variable.size()[1], target_variable.size()[0]\n",
    "    assert input_batch_size == target_batch_size\n",
    "    assert input_seq_len == target_seq_len\n",
    "\n",
    "    model.train()\n",
    "    model_hidden = model.init_hidden(input_batch_size)\n",
    "\n",
    "    model_outputs, model_hidden, model_output_lengths = model(input_variable,\n",
    "                                                            input_lengths=lengths_sorted,\n",
    "                                                            hidden = model_hidden,\n",
    "                                                            input_seqs_char = input_variable_char)\n",
    "    \n",
    "    if eval_during_train :\n",
    "        output_word_idxes = np.zeros((input_seq_len, input_batch_size), dtype=int)\n",
    "        \n",
    "    for i in range(model_outputs.size(0)): #loop over seq_len\n",
    "        loss += criterion(model_outputs[i, :, :], target_variable[i, :])\n",
    "        if eval_during_train:\n",
    "            _, topi = model_outputs.data.topk(1, dim=-1)\n",
    "            decoded_word = topi[i].cpu().numpy()\n",
    "            output_word_idxes[i,:] = decoded_word.squeeze(-1)\n",
    "\n",
    "    loss.backward()\n",
    "    ec = torch.nn.utils.clip_grad_norm(model.parameters(), parameters['clip'])\n",
    "\n",
    "    optimizer.step()\n",
    "    output_word_idxes = np.transpose(output_word_idxes, (1, 0))\n",
    "    return output_word_idxes, loss.data[0], ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no gold label sequences fed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "def evaluate(dataset_type,parameters, input_seq, output_seq, pairs, max_len, lengths, model, epoch):\n",
    "    parameters['dataset_type'] = dataset_type\n",
    "    if parameters['pred_output_dir'] is None:\n",
    "        print('has to provide a output folder for prediction results')\n",
    "        sys.exit(0)\n",
    "    \n",
    "    model = torch.load(parameters['model_path']) if model is None else model\n",
    "\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "    plot_loss_total = 0 # Reset every plot_every\n",
    "    \n",
    "    all_preds = np.zeros((len(pairs), max_len), dtype=int)\n",
    "    # all_golds = np.zeros((len(pairs), max_len))\n",
    "    all_preds_list = []\n",
    "    all_golds_list = []\n",
    "    all_preds_words = np.full((len(pairs), max_len), 's', dtype = object)\n",
    "    \n",
    "    for i_batch, ((input_variables, target_variables, input_variables_char), batch_lengths) in \\\n",
    "     enumerate(get_batch(input_seq, output_seq, pairs, \\\n",
    "     batch_size = parameters['batch_size'], max_len =max_len, lengths = lengths)):\n",
    "    \n",
    "        input_variables_sorted, input_variables_char_sorted, target_variables_sorted, \\\n",
    "        lengths_sorted, lengths_argsort \\\n",
    "            = sort_variables_lengths(input_variables, target_variables,\n",
    "                                                  batch_lengths, needs_argsort=True,\n",
    "                                                  input_variables_char=input_variables_char)\n",
    "\n",
    "        preds, attns = evaluate_step(model, input_seq, output_seq,\n",
    "                              input_variables_sorted, lengths_sorted,\n",
    "                              input_variable_char=input_variables_char_sorted)\n",
    "        # here the preds in each batch are sorted accroding to sequence length\n",
    "        new_preds = sort_variables_back(preds, lengths_argsort)\n",
    "        all_preds[i_batch * parameters['batch_size'] : \\\n",
    "                    min((i_batch + 1) * parameters['batch_size'],len(pairs)), :] = new_preds\n",
    "        all_preds_list.extend([item for sublist in new_preds.tolist() for item in sublist])\n",
    "        all_golds_list.extend([item for sublist in target_variables for item in sublist])\n",
    "    # calculate token F1 by sklearn\n",
    "    # all preds (N x seq_len) contains the predicted indices (as int)\n",
    "    \n",
    "    ### calc sklearn F1\n",
    "    new_y_pred, new_y_true, \\\n",
    "    new_label_indices, new_label_names, _, _ = remap_labels(all_preds_list,\n",
    "                                                                       all_golds_list,\n",
    "                                                                       output_seq,\n",
    "                                                                       'token')\n",
    "    ix = new_label_names.index('<SOS>')\n",
    "    new_label_names.remove('<SOS>')\n",
    "    new_label_indices.remove(new_label_indices[ix])\n",
    "    current_f1_report = metrics.classification_report(y_pred=new_y_pred, y_true=new_y_true,\n",
    "                                                    digits=4,\n",
    "                                                    labels=new_label_indices,\n",
    "                                                    target_names=new_label_names)\n",
    "    \n",
    "    current_f1_sklearn = get_sklearn_eval(current_f1_report,dataset_type)\n",
    "\n",
    "    return current_f1_sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An evaluation step (for a batch of examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eval step\n",
    "def evaluate_step(model, input_seq, output_seq, input_variable, lengths_sorted,\n",
    "                  input_variable_char=None):\n",
    "    # this one is for testing BiLSTM\n",
    "    model.eval()\n",
    "    input_batch_size, input_seq_len = input_variable.size()[1], input_variable.size()[0]\n",
    "    decoded_words = np.zeros((input_seq_len, input_batch_size), dtype=int)\n",
    "    model_outputs, model_hidden, model_output_lengths = model(input_variable,\n",
    "                                                                  input_lengths = lengths_sorted,\n",
    "                                                                  input_seqs_char=input_variable_char)\n",
    "    #loop over lengths_sorted\n",
    "    for i_batch in range(input_batch_size):\n",
    "        seq_len = lengths_sorted[i_batch]\n",
    "        for i_seq in range(seq_len):\n",
    "            topv, topi = model_outputs.data.topk(1, dim=-1)\n",
    "            decoded_word = topi[i_seq, i_batch].cpu().numpy()\n",
    "            decoded_words[i_seq, i_batch] = decoded_word\n",
    "\n",
    "    decoded_words = np.transpose(decoded_words, (1, 0))\n",
    "    return decoded_words, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_pred(preds, predfile, input_seq, output_seq, pairs):\n",
    "    '''\n",
    "    inputs and targets are torch tensors, need to be converted back to words\n",
    "    preds are already words\n",
    "    '''\n",
    "    # input_sentences = sentences_from_variables(input_lang, inputs)\n",
    "    # output_sentences = sentences_from_variables(output_lang, targets)\n",
    "    # print(len(pairs))\n",
    "    try:\n",
    "        assert len(pairs) == preds.shape[0]\n",
    "    except AssertionError:\n",
    "        print('evaluation pairs shape and preds shape are not the same!!! got:')\n",
    "        print('len(pairs)', len(pairs), 'preds.shape[0]',preds.shape[0])\n",
    "    with open(predfile, 'w') as f:\n",
    "        preds = preds.tolist()\n",
    "        \n",
    "        for i in range(len(pairs)):\n",
    "            input_seq = pairs[i][0].split(' ')\n",
    "            target_seq = pairs[i][1].split(' ')\n",
    "            pred_seq = preds[i][:len(target_seq)]\n",
    "            assert len(input_seq) == len(target_seq) and len(target_seq) == len(pred_seq)\n",
    "            for i in range(len(input_seq)):\n",
    "                line = input_seq[i] + ' ' + target_seq[i] + ' ' + pred_seq[i] + '\\n'\n",
    "                f.writelines(line)\n",
    "            f.writelines(u'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conll_eval(pred_output_filepath):\n",
    "    # subprocess.call('perl conlleval <{}'.format(pred_output_filepath), shell = True)\n",
    "    with open(pred_output_filepath, 'r') as pred_output_file:\n",
    "        p = Popen(['perl', 'conlleval'], stdin=pred_output_file, stdout=PIPE, stderr=PIPE)\n",
    "    output, err = p.communicate()\n",
    "    # rc = p.returncode\n",
    "    print(output)\n",
    "    output_lines = output.split('\\n')\n",
    "    pattern = r'(\\w+).*FB1:\\s*(\\d+.\\d+)'\n",
    "    f1_scores = {}\n",
    "    for line in output_lines:\n",
    "        match = re.search(pattern=pattern, string=line)\n",
    "        if match:\n",
    "            if match.group(1) == 'accuracy':\n",
    "                f1_scores['overall'] = float(match.group(2))\n",
    "            else:\n",
    "                f1_scores[match.group(1)] = float(match.group(2))\n",
    "    pprint(f1_scores)\n",
    "    return f1_scores\n",
    "\n",
    "def get_sklearn_eval(current_f1_report, dataset_type):\n",
    "    print('F1 (sklearn) on {:s} set:'.format(dataset_type))\n",
    "    print(current_f1_report)\n",
    "    f1_scores = {}\n",
    "    pattern = r'(\\w+)\\s+\\d\\.\\d+\\s+\\d\\.\\d+\\s+(\\d\\.\\d+)'\n",
    "    for line in current_f1_report.strip().split('\\n'):\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            if match.group(1) == 'total':\n",
    "                f1_scores['overall'] = float(match.group(2)) * 100.\n",
    "            else:\n",
    "                f1_scores[match.group(1)] = float(match.group(2)) * 100.\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "def train(parameters, input_seq, output_seq, all_pairs, all_max_len, all_lengths):\n",
    "    if parameters['use_pretrained_word_embedding']:\n",
    "        pretraind_embed, emb_size = load_pretrained_word_embeddings(parameters, input_seq)\n",
    "    else:\n",
    "        pretraind_embed = None\n",
    "        emb_size = parameters['hidden_size']\n",
    "    \n",
    "    # +1 for padding\n",
    "    blstm = BiLSTM(input_seq.n_words + 1, emb_size,\n",
    "                 parameters['hidden_size'], parameters['n_layers'],\n",
    "                 dropout=parameters['dropout_p'],\n",
    "                 pretrained_weights=pretraind_embed,\n",
    "                 output_size=len(output_seq.words) + 1,\n",
    "                 use_char_embedding=parameters['use_char_embedding'],\n",
    "                 char_alphabet_size=input_seq.n_chars + 1,\n",
    "                 char_emb_size=parameters['char_emb_size'])\n",
    "    if use_cuda:\n",
    "        blstm.cuda()\n",
    "    print(blstm)\n",
    "\n",
    "    optimizer = optim.SGD(blstm.parameters(), lr=parameters['learning_rate'], momentum=0.8)\n",
    "#     criterion = nn.NLLLoss(ignore_index=0)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # Keep track of time elapsed and running averages\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    ecs, dcs = [], []\n",
    "    eca, dca = 0, 0\n",
    "    print_loss_total = 0\n",
    "    all_output_word_idxes = np.zeros((len(all_pairs['train']), all_max_len['train']))\n",
    "    \n",
    "    f1s = defaultdict(dict)\n",
    "    for dataset_type in ['train', 'valid', 'test']:\n",
    "        f1s[dataset_type]['overall'] = []\n",
    "        \n",
    "    best_valid_f1 = 0.0\n",
    "    patience_cnt = 0\n",
    "    for epoch in range(1, parameters['n_epochs'] + 1):\n",
    "        print('\\nStarting epoch {:d}...'.format(epoch))\n",
    "        epoch_start_time = time.time()\n",
    "        for i_batch, ((input_variables, target_variables, input_variables_char), batch_lengths) in \\\n",
    "                enumerate(get_batch(input_seq, output_seq, all_pairs['train'], \\\n",
    "                                    batch_size=parameters['batch_size'],\n",
    "                                    max_len=all_max_len['train'], lengths=all_lengths['train'])):\n",
    "\n",
    "            input_variables_sorted, input_variables_char_sorted, target_variables_sorted, \\\n",
    "            lengths_sorted, lengths_argsort = sort_variables_lengths(input_variables, target_variables,\n",
    "                                                      batch_lengths, needs_argsort = True,\n",
    "                                                      input_variables_char = input_variables_char)\n",
    "\n",
    "            output_word_idxes, loss, ec = train_step(parameters, output_seq, input_variables_sorted,\n",
    "                                                          target_variables_sorted, lengths_sorted,\n",
    "                                                          blstm, optimizer, criterion,\n",
    "                                                          input_variable_char = input_variables_char_sorted)\n",
    "\n",
    "            output_word_idxes_sorted = sort_variables_back(output_word_idxes, lengths_argsort)\n",
    "            all_output_word_idxes[i_batch * parameters['batch_size']: \\\n",
    "                min((i_batch + 1) * parameters['batch_size'], len(all_pairs['train'])), :] = output_word_idxes_sorted\n",
    "            print_loss_total += loss\n",
    "\n",
    "            eca += ec\n",
    "\n",
    "        if epoch == 0: continue\n",
    "        else:\n",
    "#             print_loss_avg = print_loss_total / parameters['print_every']\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start,\n",
    "                                                              epoch / float(parameters['n_epochs'])),\n",
    "                                                   epoch, epoch / parameters['n_epochs'] * 100,\n",
    "                                                   print_loss_total)\n",
    "            print_loss_total = 0\n",
    "            print(print_summary)\n",
    "\n",
    "            for dataset_type in ['train', 'valid', 'test']:\n",
    "                output_word_idxes = all_output_word_idxes if dataset_type == 'train' else None\n",
    "                print('evaluating on {:s} set ...'.format(dataset_type))\n",
    "                \n",
    "                current_f1 = evaluate_during_train(dataset_type, parameters,\n",
    "                                                   input_seq, output_seq,\n",
    "                                                   all_pairs[dataset_type],\n",
    "                                                   all_max_len[dataset_type],\n",
    "                                                   all_lengths[dataset_type],\n",
    "                                                   model=blstm,\n",
    "                                                   epoch=epoch,\n",
    "                                                   output_word_idxes = output_word_idxes)\n",
    "                f1s[dataset_type]['overall'].append(current_f1['overall'])\n",
    "\n",
    "            current_valid_f1 = f1s['valid']['overall'][epoch-1]\n",
    "\n",
    "            if current_valid_f1 > best_valid_f1:\n",
    "                best_valid_f1 = current_valid_f1\n",
    "                model_path = save_best(blstm, parameters['output_model_dir'])\n",
    "                print(\"Saved model in {:s}\".format(parameters['output_model_dir']))\n",
    "                parameters['model_path'] = model_path\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                print('The valid F1 does not improve in the last {:d} epochs.'.format(patience_cnt+1))\n",
    "                patience_cnt += 1\n",
    "\n",
    "        epoch_elapsed_training_time = time.time() - epoch_start_time\n",
    "\n",
    "        print('Training completed in {0:.2f} seconds'.format(epoch_elapsed_training_time))\n",
    "        if patience_cnt >= parameters['patience']:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    # finishing up the experiments\n",
    "    f1s_for_plot = {}\n",
    "    for dataset_type in ['train', 'valid', 'test']:\n",
    "        f1s_for_plot[dataset_type] = f1s[dataset_type]['overall']\n",
    "    graph_path = os.path.join(parameters['output_model_dir'], 'F1-plot.svg')\n",
    "    plot_f1(f1s_for_plot, graph_path, 'step'+str(parameters['step']))\n",
    "    print('F1 (sklearn):')\n",
    "    pprint(f1s)\n",
    "#     return best_valid_f1, f1s\n",
    "\n",
    "\n",
    "def evaluate_during_train(dataset_type, parameters, input_seq, output_seq,\n",
    "                          pairs, max_len, lengths, model=None, epoch=0, output_word_idxes = None):\n",
    "\n",
    "    current_f1 = evaluate(dataset_type, parameters, input_seq, output_seq,\n",
    "                 pairs, max_len, lengths, model, epoch)\n",
    "\n",
    "    return current_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_best(model, save_path):\n",
    "    model_path = os.path.join(save_path, 'best_model.pt')\n",
    "    save_model(model, model_path)\n",
    "    return model_path\n",
    "\n",
    "def save_model(model, filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        open(filename, 'w').close()\n",
    "    torch.save(model, filename)\n",
    "    print('Saved %s as %s' % (model.__class__.__name__, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from file /home/liah/word_vectors/eng/glove.6B.100d.txt...\n",
      " Embedding size: 100\n",
      "Map 17616 tokens with pretrained embeddings.\n",
      "direct map: 8840\n",
      "lower-case map: 8776\n",
      "\n",
      "Randomly initialized 323 token embeddings.\n",
      "1151 low_frequency_word: \n",
      "BiLSTM (\n",
      "  (embedding): Embedding(19092, 100)\n",
      "  (char_embedding): Embedding(77, 25)\n",
      "  (char_lstm): LSTM(25, 25, dropout=0.1, bidirectional=True)\n",
      "  (dropout): Dropout (p = 0.5)\n",
      "  (lstm): LSTM(150, 128, dropout=0.1, bidirectional=True)\n",
      "  (out1): Linear (256 -> 128)\n",
      "  (out2): Linear (128 -> 14)\n",
      "  (softmax): LogSoftmax ()\n",
      ")\n",
      "\n",
      "Starting epoch 1...\n",
      "0m 42s (- 139m 39s) (1 0%) 806.6310\n",
      "evaluating on train set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (sklearn) on train set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC     0.9444    0.0345    0.0666      2954\n",
      "       MISC     0.0000    0.0000    0.0000      1633\n",
      "        PER     0.7833    0.6968    0.7375      3928\n",
      "\n",
      "avg / total     0.6890    0.3334    0.3633      8515\n",
      "\n",
      "evaluating on valid set ...\n",
      "F1 (sklearn) on valid set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC     0.9029    0.0444    0.0847      2094\n",
      "       MISC     0.0000    0.0000    0.0000      1268\n",
      "        PER     0.8196    0.7272    0.7707      3149\n",
      "\n",
      "avg / total     0.6868    0.3660    0.3999      6511\n",
      "\n",
      "evaluating on test set ...\n",
      "F1 (sklearn) on test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC     0.9254    0.0322    0.0622      1925\n",
      "       MISC     0.0000    0.0000    0.0000       918\n",
      "        PER     0.8447    0.6946    0.7623      2773\n",
      "\n",
      "avg / total     0.7343    0.3540    0.3977      5616\n",
      "\n",
      "Saved BiLSTM as /home/liah/ner/seq2seq_for_ner/src/model/tutorial/best_model.pt\n",
      "Saved model in /home/liah/ner/seq2seq_for_ner/src/model/tutorial\n",
      "Training completed in 240.81 seconds\n",
      "\n",
      "Starting epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type BiLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4m 42s (- 466m 2s) (2 1%) 204.7604\n",
      "evaluating on train set ...\n"
     ]
    }
   ],
   "source": [
    "train(parameters, input_seq, output_seq, all_pairs, all_max_len, all_lengths)\n",
    "#an epoch (train+eval) takes about 3 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
